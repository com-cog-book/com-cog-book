{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ADALINE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adaline algorithm (Adaptive Linear Algorithm) was proposed in 1959, shortly after Rosenblattâ€™s perceptron, by **Bernard Widrow** and **Ted Hoff** (one of the inventors of the microprocessor) at Stanford. Widrow and Hoff were electrical engineers, yet Widrow had attended the famous *Dartmouth Summer Research Project on Artificial Intelligence* in 1956, experience that got him interested in the idea of building brain-like artificial learning systems. When Widrow moved from MIT to Stanford, a colleague asked him whether he would be interested in taking Ted Hoff as his doctoral student. Widrow and Hoff came up with the Adaline algorithm on a Friday during their first session working together. At that time, implementing an algorithm in a mainframe computer was slow and expensive, so they decided to build a small device capable of being trained by the Adaline algorithm to learn to classify patterns of inputs.  \n",
    "\n",
    "The main difference between the perceptron and Adaline, is that Adaline works by minimizin the **sum of squares of the linear errors** over a training set. This means that the learning rule is based on a **linear activation function** rather than a unit step function as in the perceptron. This is important as allows the minimization of a continuous cost function. Continuous cost functions have the advantage of being differentiable, which allows training neural nets by using the chain rule of calculus, which opened the door to train more complex algorithms like non-linear multilayer perceptrons, logistic regression, support vector machines, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal definition of ADALINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, Adaline uses a **linear activation function**, which is essencially the **identity function** of the net input to the network. This is defined as:\n",
    "\n",
    "$$\\hat{y} = \\sum_{j=1}^n x_j w_j + \\theta$$\n",
    "\n",
    "\n",
    "where:  \n",
    "- $\\hat{y}$ is the output of the model (real value scalar)  \n",
    "- $x$ is a real value input vector   \n",
    "- $w$ is a real vaue weight vector\n",
    "- $\\theta$ is a bias term  \n",
    "\n",
    "It is crucial to notice that even when we use a linear activation function to compute the output of the network, if we attempt to make a **binary classification decision**, we will still use a step-like decision function by taking the sign as: \n",
    "\n",
    "$$  sgn(\\hat{y}) =\n",
    "\\begin{cases}\n",
    " +1,  & \\text{if $\\hat{y}$ > 0} \\\\\n",
    "-1, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the Perceptron and Adaline fundamental difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may be wondering what's the difference between the Perceptron and Adaline considering that both end up using a step-function to make classifications. The difference is the **learning rule to update the weight** of the network. The perceptron update the weights by computing the difference between the expected and predicted **class labels**. In practice, this means that the perceptron is comparing three types of discrete values: -1, 0, and 1. On the other hand, Adaline computes the difference between the expected class label (i.e., -1 or 1) and the **continious real value** of the linear activation function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, the Adaline learning rule consist of comparing the expected class label to the predicted continous real value output. To achieve this, Adaline uses the **LMS (least mean square) algorithm**, also know as **Widrow-Hoff Delta Rule**, which minimize the sum of squares of the linear errors over the training set. In the machine learning literature, this is know as a **cost funtion** to be minimized. This is defined as:\n",
    "\n",
    "$$L = \\sum_{j=1}^n (\\hat{y}_j-y_j)^2$$\n",
    "\n",
    "where: \n",
    "- $\\hat{y}$ is the output of the model (real value scalar)  \n",
    "- $y$ is the expected class label (-1 or +1)\n",
    "\n",
    "Now the question is how to minimize sum of squares erros (SSE). We do this by adjusting the values of $w$ vector. Since we are working with a continous value function, we can compute the change in the SSE with respect to changes in $w$ by applying the **gradient descent algorithm**. Therefore, we update the values of $w$ by:\n",
    "\n",
    "$$w_{j+1} \\leftarrow w_j + \\eta(- \\Delta_j)$$\n",
    "\n",
    "where:\n",
    "- $\\eta$ is the learning rate (positive constant)\n",
    "- $\\Delta_j$ is the value of the gradient at a point in the SSE surface \n",
    "\n",
    "This algorithm works by taking steps of a size controled by the learning rate $\\eta$, on the surface defined by the vector of weights. A common way to express this idea is in analogy to climbing: if you're in a mountain, you can ascent by **climbing up-hill** or descent by **climbing down-hill**. Since the surface defined by this quadratic function is convex (think in a bowl) and has a unique global minimun, **we want to go down-hill** (i.e., we do *gradient descent*) where the SSE is minimized.  \n",
    "\n",
    "To obtain the gradient in a given point the convex-surface, we compute the partial derivative of the cost function $L$ with respect to each weight in the weight vector as:\n",
    "\n",
    "$$\\frac{\\partial L} {\\partial w_j} = -\\sum_{i}(y_i - \\hat{y_i})x_{ji}$$ \n",
    "\n",
    "Finally, by replacing terms, the update rule can be writen as:\n",
    "\n",
    "$$\\Delta w_j = -\\eta \\frac{\\partial L} {\\partial w_j} = \\eta\\sum_{i}(y_i - \\hat{y_i})x_{ji}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the Adaline algorithm from scrath with Python and Numpy (a Python package for scientific computing). The goal is to understand the perceptron step-by-step execution rather than achieving an elegant implementation. I'll break down each step into functions to ensemble everything at the end. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vector of random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_weights(X, random_state: int):\n",
    "    '''create vector of random weights\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: 2-dimensional array, shape = [n_samples, n_features]\n",
    "    Returns\n",
    "    -------\n",
    "    w: array, shape = [w_bias + n_features]'''\n",
    "    rand = np.random.RandomState(random_state)\n",
    "    w = rand.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions from Adaline are obtained by a linear combination of features and weights. It is common practice to begin with a vector of small random weights that would be updated later by the Adaline learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_input(X, w):\n",
    "    '''Compute net input as dot product'''\n",
    "    return np.dot(X, w[1:]) + w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass the featue matrix and the previously generated vector of random weights to compute the inner product. Remember that we need to add an extra weight for the bias term at the begining of the vector (`w[0`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(X):\n",
    "    '''Compute linear activation'''\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the activation function returns the same values passed in. As we mentioned earlier, the linear activation function of Adaline, is the **identity function**, which means exactly this: units will be activated in direct proportion to the output of the linear combination of vectors and weights. Technically, we might not use this function and the result will be the same. Yet, we add this for conceptual completeness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    '''Return class label after unit step'''\n",
    "    return np.where(net_input(X, w) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that although Adaline learning rule works by comparing the output of a linear function against the class labels, when doing predictions, we still need to pass the output by a *sgn function* to get class labels as in the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop - Learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, eta=0.001, n_iter=100):\n",
    "    '''loop over exemplars and update weights'''\n",
    "    costs = []\n",
    "    w = random_weights(X, random_state=1)\n",
    "    for i in range(n_iter):\n",
    "        net_input_v = net_input(X, w)\n",
    "        output = activation(net_input_v) # identity function\n",
    "        errors = (y - output) # compute errors for the entire dataset\n",
    "        w[1:] += eta * X.T @ errors # update weigths for the entire dataset (feature weights)\n",
    "        w[0] += eta * errors.sum() # update weigths for the entire dataset (bias-term weights)\n",
    "        cost = (errors**2).sum() / 2.0 \n",
    "        costs.append(cost)\n",
    " \n",
    "    return w, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the fit method that implements the Adaline learning rule:\n",
    "\n",
    "* Create a vector of random weights by using the `random_weights` function with dimensionality equal to the number of columns in the feature matrix\n",
    "* Loop over the entire dataset `n_iter` times with `for i in range(n_iter)`\n",
    "* Compute the inner product between the feature matrix $X$ and the weight vector $w$ by using the `net_input(X, w)` function\n",
    "* Compute the difference between the predicted values and the target values for the entire dataset `(y - output)`\n",
    "* Update the weights in proportion to the learning rate $\\eta$ by `w[1:] += eta * X.T @ errors` and `w[0] += eta * errors.sum()`\n",
    "* Compute the SSE `cost = (errors**2).sum() / 2.0 `\n",
    "* Save the SSE for each iteration `costs.append(cost)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the Adaline implementation on the same test that in our previous perceptron: **classifying figures by their shape**. We'll create two type of figures: **tall-figures** and **wide-figures**. As the name suggest, the tall-figures are figures that are taller than wider, and the wider-figures are figures that are wider than taller. \n",
    "\n",
    "To accomplish this, we'll sample tall and wide figures at random from a normal distribution by using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create figures type\n",
    "def figure_type(mu1, sigma1, mu2, sigma2, n_samples, target, seed):\n",
    "    '''creates [n_sampes, 2] array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mu1, sigma1: int, shape = [n_samples, 2]\n",
    "        mean feature-1, standar-dev feature-1\n",
    "    mu2, sigma2: int, shape = [n_samples, 2]\n",
    "        mean feature-2, standar-dev feature-2\n",
    "    n_samples: int, shape= [n_samples, 1]\n",
    "        number of sample cases\n",
    "    target: int, shape = [1]\n",
    "        target value\n",
    "    seed: int\n",
    "        random seed for reproducibility\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    X: ndim-array, shape = [n_samples, 2]\n",
    "        matrix of feature vectors\n",
    "    y: 1d-vector, shape = [n_samples, 1]\n",
    "        target vector\n",
    "    ------\n",
    "    X'''\n",
    "    rand = np.random.RandomState(seed)\n",
    "    f1 = rand.normal(mu1, sigma1, n_samples)\n",
    "    f2 = rand.normal(mu2, sigma2, n_samples)\n",
    "    X = np.array([f1, f2])\n",
    "    X = X.transpose()\n",
    "    y = np.full((n_samples), target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (100, 2) \n",
      "target vector shape: (100,)\n",
      "Feature matrix: \n",
      "[[13.24869073  4.55287144]\n",
      " [ 8.77648717  6.2245077 ]\n",
      " [ 8.9436565   5.40349164]\n",
      " [ 7.85406276  5.59357852]] \n",
      "target vector: \n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# create tall-figures matrix\n",
    "T, y_t = figure_type(10, 2, 5, 1, 100, 1, 1)\n",
    "nl = '\\n'\n",
    "print(f'Feature matrix shape: {T.shape} {nl}target vector shape: {y_t.shape}')\n",
    "print(f'Feature matrix: {nl}{T[0:4, :]} {nl}target vector: {nl}{y_t[0:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (100, 2) \n",
      "target vector shape: (100,)\n",
      "Feature matrix: \n",
      "[[ 4.58324215 12.32304298]\n",
      " [ 4.94373317 10.7721561 ]\n",
      " [ 2.8638039   7.73373345]\n",
      " [ 6.64027081 10.86618511]] \n",
      "target vector: \n",
      "[-1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# create wide-figures matrix\n",
    "W, y_w = figure_type(5, 1, 10, 2, 100, -1, 2)\n",
    "nl = '\\n'\n",
    "print(f'Feature matrix shape: {W.shape} {nl}target vector shape: {y_w.shape}')\n",
    "print(f'Feature matrix: {nl}{W[0:4, :]} {nl}target vector: {nl}{y_w[0:4]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(T[:, 1], T[:, 0], color='red', marker='o', label='tall-figures')\n",
    "plt.scatter(W[:, 1], W[:, 0], color='blue', marker='x', label='wide-figures')\n",
    "plt.xlabel('width')\n",
    "plt.ylabel('height')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.concatenate((T, W), axis=0)\n",
    "y = np.concatenate((y_t, y_w), axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (140, 2)\n",
      "y_train.shape: (140,)\n",
      "X_test.shape: (60, 2)\n",
      "y_test.shape: (60,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train.shape: {X_train.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'X_test.shape: {X_test.shape}')\n",
    "print(f'y_test.shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the training set, training targets, learning rate, and number of iterations. Recall that the Adaline update the weights **after** doing a full pass for the entire training set, whereas the peceptron does it after each individual training exemplar. Therefore, we will pass the entire training set 1 time (`n_iter=1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector of weights: [ 0.02614765  0.42163998 -0.2764838 ]\n",
      "\n",
      "cost at each time step: [70.66186787674164]\n",
      "\n",
      "predicted value for each case: [ 1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1\n",
      "  1 -1  1  1  1  1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1 -1  1\n",
      " -1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit and predict values\n",
    "def training_values(X_train, y_train, eta=0.001, n_iter=1):\n",
    "    w, cost = fit(X_train, y_train, eta=eta, n_iter=n_iter)\n",
    "    y_pred = predict(X_test, w)\n",
    "    return w, cost, y_pred\n",
    "\n",
    "w, cost, y_pred = training_values(X_train, y_train)\n",
    "\n",
    "nl = '\\n'\n",
    "print(f'vector of weights: {w}{nl}')\n",
    "print(f'cost at each time step: {cost}{nl}')\n",
    "print(f'predicted value for each case: {y_pred}{nl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "def acc(y_pred, y_test):\n",
    "    num_correct_predictions = (y_pred == y_test).sum()\n",
    "    accuracy = (num_correct_predictions / y_test.shape[0]) * 100\n",
    "    return (accuracy)\n",
    "    \n",
    "accuracy = acc(y_pred, y_test)\n",
    "print('Test set accuracy: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a SSE (cost) equal to **70.7** with a **93.33%** of classification accuracy. Pretty good. Let's try now by passing training set **twice times** by the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector of weights: [-0.12486643 -1.09800089 -1.08582971]\n",
      "\n",
      "cost at each time step: [70.66186787674164, 158.3678895769619]\n",
      "\n",
      "predicted value for each case: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit and predict values\n",
    "w, cost, y_pred = training_values(X_train, y_train, n_iter=2)\n",
    "\n",
    "nl = '\\n'\n",
    "print(f'vector of weights: {w}{nl}')\n",
    "print(f'cost at each time step: {cost}{nl}')\n",
    "print(f'predicted value for each case: {y_pred}{nl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = acc(y_pred, y_test)\n",
    "print('Test set accuracy: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something really odd happened: the **SSE went up from 70 to 158.4**, and the **accuracy drop from 93.33% to 50%**. All the test cases has been classified as -1. Maybe we need more training, let's try with 5 loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector of weights: [ 553.09756457 4250.98926834 4373.14848469]\n",
      "\n",
      "cost at each time step: [70.66186787674164, 158.3678895769619, 20245.308891118297, 5041394.106643893, 1260698508.9443183]\n",
      "\n",
      "predicted value for each case: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Test set accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# fit and predict values\n",
    "w, cost, y_pred = training_values(X_train, y_train, n_iter=5)\n",
    "\n",
    "nl = '\\n'\n",
    "print(f'vector of weights: {w}{nl}')\n",
    "print(f'cost at each time step: {cost}{nl}')\n",
    "print(f'predicted value for each case: {y_pred}{nl}')\n",
    "\n",
    "accuracy = acc(y_pred, y_test)\n",
    "print('Test set accuracy: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exploding gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing improved, on the contrary, the SSE is getting bigger with each iteration over the training set. The key to this problem is on the **vector of weights**. Remember that the weights are updated in relation to the **mistmatch between the net ouput of the linear activation function and the class labels**. This means that unless the error is exactly zero, **the weights will continue to grow (or decrease) without limit** after each pass of the training set. As the weights grow,the net output will continue to grow as well, making the error on each iteration bigger. This loop continues **until the weights \"explode\"** and the network is unable to learn anything meaningful anymore. This is know as the \"exploding\" gradient problem, which is one of the main limitations on the use of this kind of training algorithms in the context of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Linear Separability Constrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Widrow, B., & Hoff, M. E. (1960). Adaptive switching circuits (No. TR-1553-1). Stanford Univ Ca Stanford Electronics Labs.\n",
    "\n",
    "- Widrow, B., & Lehr, M. A. (1990). 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proceedings of the IEEE, 78(9), 1415-1442.\n",
    "\n",
    "- Widrow, B., & Lehr, M. A. (1995). Perceptrons, Adalines, and backpropagation. The handbook of brain theory and neural networks, 719-724.\n",
    "\n",
    "**For code implementation:** \n",
    "\n",
    "- Raschka, S. (2015). Python machine learning. Packt Publishing Ltd."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
