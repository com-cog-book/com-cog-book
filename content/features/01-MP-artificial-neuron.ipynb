{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The McCulloch-Pitts Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alan Turing's formalization of computation as [Turing Machines](https://www.youtube.com/watch?v=dNRDvLACg5Q) provided the theoretical and mathematical foundations for modern computer science. Turing Machines are a construct, an abstraction of a general computation device that helps to reveal the extent and limitations of what can be computed. Every modern computer device can be described as a Turing Machine, regardless of the details of its physical implementation[<sup>1</sup>](#fn1). This foundation inspired whole generation of cognitive scientist to develop computational models of cognition, among them, [Warren McCulloch](https://en.wikipedia.org/wiki/Warren_McCulloch) and [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts), who in 1943 first proposed that neuronal activity in the brain can be represented as computational devices in resemblance to Turing Machines in their article \"*A Logical Calculus of the Ideas Immanent in Nervous Activity*\".\n",
    "\n",
    "The hearth of the McCulloch and Pitts idea is that given the *all-or-none* character of neural activity, the behavior of the nervous system can be described by means of *propositional logic*. To understand this, let's examine that main components of biological neurons:\n",
    "1. the **cell body** containing the nucleus and metabolic machinery\n",
    "2. an **axon** that transmit information via its synaptic terminals, and \n",
    "1. the **dendrites** that receive inputs from other neurons via synapses. \n",
    "\n",
    "**Figure 1** shows an abstract scheme of the main components of two neurons and their synapses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"fn1\"> Footnote 1: examining the definition of a Turin Machine is beyond the scope of this tutorial. For an extended explanation of Turing Machines see https://plato.stanford.edu/entries/turing-machine/#DefiTuriMach </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 1<center/>\n",
    "<br style = “line-height:100px;”>\n",
    "<img src=\"images/neuron_synapse.svg\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons communicate with each other by passing *electro-chemical signals* from the axon terminals in the pre-synaptic neuron to the dendrites in the post-synaptic neuron. Usually, each neuron connects to hundreds or thousands of neurons. For a neuron to \"*fire*\", certain *threshold* of electrochemical excitation must be passed. The *combined excitatory and inhibitory input* received by the post-synaptic neuron from the pre-synaptic neurons determines whether the neuron *passes such threshold and fires*. Is this *firing* or *spiking* behavior that McCulloch and Pitts modeled computationally. Furthermore, by carefully calibrating the magnitude of the inhibitory and excitatory signals passed to a neuron, McCulloch and Pitts were able to emulate the behavior of a few *boolean functions* or *logical gates*, like the *AND*, *OR*, and *NOR*. If you think in this process abstractly, neurons are biological computational devices as well in the sense that they can receive inputs, apply calculations over those inputs algorithmically, and produce outputs.\n",
    "\n",
    "At this point you may be thinking \"*Hold on, this is a model of the function of a single neuron, not of a cognitive process*\", and you would be correct. Nevertheless, there a few good reasons why we study the McCulloch and Pitts model as a seminal work in computational models of cognition. This model was the first stepping-stone in the development of later more complex computational models of cognition, like the *perceptron* and *multilayer-perceptron*. Additionally, as we mentioned before, McCulloch and Pitts also showed that such a model could be used to represent boolean functions. Boolean functions are essentially about logic, and logical reasoning is one of the main topics of study in cognitive sciences. In a way, the McCulloch and Pitts artificial neuron provided one of the first clues about how to study logic and reasoning in humans from a computational perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch and Pitts developed a mathematical formulation know as *linear threshold gate*, which describes the activity of a single neuron with two states, *firing* or *not-firing*. In its simple form, the mathematical formulation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum = \\sum_{i=1}^NI_iW_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y(Sum)=\n",
    "\\begin{cases}\n",
    "1, & \\text{if } Sum \\geq T \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where  $I_1, I_2,..., I_N$ are binary input values  $\\in\\{0,1\\}$ ;  $W_1, W_2,..., W_N$ are weights associated with each input $\\in\\{-1,1\\}$ ; $Sum$ is the weighted sum of inputs; and $T$ is a predefined threshold value for the neuron activation (i.e., *firing*).  An input is considered *excitatory* when its contribution to the weighted sum is positive, for instance $I_1*W_1 = 1 * 1 = 1$; whereas an input is considered *inhibitory* when its contribution to the weighted sum is negative, for instance $I_1*W_1 = 1 * -1 = -1$. If the value of $Sum$ is $\\geq$ $T$, the neuron fires, otherwise, it does not.  **Figure 2** display a graphical representation of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 2</center>\n",
    "<br style = “line-height:100px;”>\n",
    "<center><img src=\"images/linear_threshold_function_crop.svg\" width=\"60%\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as a *step-function*, where the $y$-axis encodes the activation-state of the neuron, and the $Sum$-axis encodes the output of the weighted sum of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the McCulloch-Pitts artificial neuron in code is very simple thanks to the features and libraries of high-level programming languages that are available today. We can do this in four steps using `python` and `numpy`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: generate a vector of inputs and a vector of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector:[0 1 1], Weight vector:[-1  1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(seed=0)\n",
    "I = np.random.choice([0,1], 3)# generate random vector I, sampling from {0,1}\n",
    "W = np.random.choice([-1,1], 3) # generate random vector W, sampling from {-1,1} \n",
    "print(f'Input vector:{I}, Weight vector:{W}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: compute the dot product between the vector of inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product: 2\n"
     ]
    }
   ],
   "source": [
    "dot = I @ W\n",
    "print(f'Dot product: {dot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: define the threshold activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_threshold_gate(dot: int, T: float) -> int:\n",
    "    '''Returns the binary threshold output'''\n",
    "    if dot >= T:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: compute the output based on the threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: 1\n"
     ]
    }
   ],
   "source": [
    "T = 1\n",
    "activation = linear_threshold_gate(dot, T)\n",
    "print(f'Activation: {activation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, the threshold was set to $T=1$. Since $Sum=2$, the neuron fires. If we increase the threshold for firing to  $T=3$, the neuron will not fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: 0\n"
     ]
    }
   ],
   "source": [
    "T = 3\n",
    "activation = linear_threshold_gate(dot, T)\n",
    "print(f'Activation: {activation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Boolean Algebra Using the McCulloch-Pitts Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how logical thinking works has been one of the main goals of cognitive scientists since the creation of the field. One way to approach the study of logical thinking is by building an artificial system able to perform logical operations. *Truth tables* are a schematic way to express the behavior of *boolean functions*, which are essentially logical operations. Here, we will use the McCulloch-Pitts artificial neuron model to replicate the behavior of a few boolean functions, as expressed in their respective truth tables. Notice that I'm using the term \"function\" to describe boolean logic, but you may find that the term \"logic gate\" is also widely used, particularly in the electronic circuits literature where this kind of function is fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### The AND Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *AND* function is \"activated\" only when all the incoming inputs are \"on\", this is, it outputs a 1 only when all inputs are 1. In \"neural\" terms, the neuron *fires* when the incoming signals are both *excitatory*. On a more abstract level, think in a situation where you would decide that something is \"true\" or you would say \"yes\", depending on the state of conditions. This relationship is expressed in **Table 1** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 1: Truth Table For AND Function</center>\n",
    "\n",
    "| A | B | Output |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 0      |\n",
    "| 1 | 0 | 0      |\n",
    "| 1 | 1 | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that you are deciding whether to watch a movie or not. In this simplified scenario, you would watch the movie only if the movie features Samuel L. Jackson AND the director is Quentin Tarantino. Now the truth table looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 2: Movie Decision Table</center>\n",
    "\n",
    "| Samuel L. Jackson | Quentin Tarantino | Watch the movie |\n",
    "|-------------------|-------------------|-----------------|\n",
    "| No                | No                | No              |\n",
    "| No                | Yes               | No              |\n",
    "| Yes               | No                | No              |\n",
    "| Yes               | Yes               | Yes             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, the AND function can be implemented with the MP artificial neuron model. Each neuron has four parts: inputs, weights, threshold, and output. The *inputs* are given in the Movie Decision Table, and the *output* is completely determined by other elements, therefore, to create an AND function, we need to manipulate the *weights* and the *threshold*. Since we want the neuron to fire only when both inputs are excitatory, the threshold for activation must be 2. To obtain an output of 2, we need both inputs to be excitatory, therefore, the weights must be positive (i.e., 1). Summarizing, we need: \n",
    "\n",
    "- weights: all positive\n",
    "- threshold: 2\n",
    "\n",
    "Now, let's repeat the same four steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: generate a vector of inputs and a vector of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input table:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "# matrix of inputs\n",
    "input_table = np.array([\n",
    "    [0,0], # both no\n",
    "    [0,1], # one no, one yes\n",
    "    [1,0], # one yes, one no\n",
    "    [1,1]  # bot yes\n",
    "])\n",
    "\n",
    "print(f'input table:\\n{input_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [1 1]\n"
     ]
    }
   ],
   "source": [
    "# array of weights\n",
    "weights = np.array([1,1])\n",
    "print(f'weights: {weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: compute the dot product between the matrix of inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot products: [0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# dot product matrix of inputs and weights\n",
    "dot_products = input_table @ weights\n",
    "print(f'Dot products: {dot_products}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in case you are not sure why multiplying a 4x2 matrix by a 1x2 vector works, the answer is that `numpy` internally \"broadcast\" the smaller array to match the shape of the larger array. This means that the 1x2 is transformed into a 4x2 array where each new row replicates the values of the original 1x2 array. More on broadcasting [here](https://numpy.org/doc/1.18/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: define the threshold activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined this already, so we will reuse our `linear_threshold_gate` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: compute the output based on the threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: 0\n",
      "Activation: 0\n",
      "Activation: 0\n",
      "Activation: 1\n"
     ]
    }
   ],
   "source": [
    "T = 2\n",
    "for i in range(0,4):\n",
    "    activation = linear_threshold_gate(dot_products[i], T)\n",
    "    print(f'Activation: {activation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, only the last movie, with Samuel L. Jackson as an actor and Quentin Tarantino as director, resulted in an activated neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### The OR Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *OR* function is \"activated\" when *at least one* of the incoming inputs is \"on\". In \"neural\" terms, the neuron *fires* when at least one of the incoming signals is *excitatory*. This relationship is expressed in **Table 3** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 3: Truth Table For OR Function</center>\n",
    "\n",
    "| A | B | Output |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0      |\n",
    "| 0 | 1 | 1      |\n",
    "| 1 | 0 | 1      |\n",
    "| 1 | 1 | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you decide to be flexible about your decision criteria. Now, you will watch the movie if at least one of your favorite stars, Samuel L. Jackson or Quentin Tarantino, is part of the movie. Now the truth table looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 4: Movie Decision Table</center>\n",
    "\n",
    "| Samuel L. Jackson | Quentin Tarantino | Watch the movie |\n",
    "|-------------------|-------------------|-----------------|\n",
    "| No                | No                | No              |\n",
    "| No                | Yes               | Yes             |\n",
    "| Yes               | No                | Yes             |\n",
    "| Yes               | Yes               | Yes             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the neuron to fire when at least one of the inputs is excitatory (Samuel or Quentin is part of the movie), \n",
    "the threshold for activation must be 1. To obtain an output of at least 1, we need both inputs to be excitatory, therefore, the weights must be positive (i.e., 1). Summarizing, we need: \n",
    "\n",
    "- weights: all positive\n",
    "- threshold: 1\n",
    "\n",
    "Now, let's repeat the same four steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: generate a vector of inputs and a vector of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the matrix of inputs nor the array of weights changes, so we can reuse our `input_table` and `weights` vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: compute the dot product between the matrix of inputs and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neither the matrix of inputs nor the array of weights changes, the dot product of those stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: define the threshold activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is defined already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: compute the output based on the threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: 0\n",
      "Activation: 1\n",
      "Activation: 1\n",
      "Activation: 1\n"
     ]
    }
   ],
   "source": [
    "T = 1\n",
    "for i in range(0,4):\n",
    "    activation = linear_threshold_gate(dot_products[i], T)\n",
    "    print(f'Activation: {activation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably appreciate by now, the only thing we needed to change was the `threshold` value, and the expected behavior is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### The NOR function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *OR* function is \"activated\" when *all* the incoming inputs are \"off\". In this sense, it is the inverse of the OR function. In \"neural\" terms, the neuron *fires* when all the signals are *inhibitory*. This relationship is expressed in **Table 5** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 5: Truth Table For OR Function</center>\n",
    "\n",
    "| A | B | Output |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 1      |\n",
    "| 0 | 1 | 0      |\n",
    "| 1 | 0 | 0      |\n",
    "| 1 | 1 | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, imagine that you got saturated of watching Samuel L. Jackson and/or Quentin Tarantino movies, and you decide you only watch movies where both are absent. The presence of even one of them is unacceptable for you. The new truth table looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table 6: Movie Decision Table</center>\n",
    "\n",
    "| Samuel L. Jackson | Quentin Tarantino | Watch the movie |\n",
    "|-------------------|-------------------|-----------------|\n",
    "| No                | No                | yes             |\n",
    "| No                | Yes               | no              |\n",
    "| Yes               | No                | no              |\n",
    "| Yes               | Yes               | no              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the neuron to fire only when both inputs are inhibitory, the threshold for activation must be 0. To obtain an output of 0, we need both inputs to be inhibitory, therefore, the weights must be negative (i.e., -1). Summarizing, we need: \n",
    "\n",
    "- weights: all negative\n",
    "- threshold: 0\n",
    "\n",
    "Now, let's repeat the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: generate a vector of inputs and a vector of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of inputs remain the same, but we need a new vector of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-1 -1]\n"
     ]
    }
   ],
   "source": [
    "# array of weights\n",
    "weights = np.array([-1,-1])\n",
    "print(f'weights: {weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: compute the dot product between the matrix of inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot products: [ 0 -1 -1 -2]\n"
     ]
    }
   ],
   "source": [
    "# dot product matrix of inputs and weights\n",
    "dot_products = input_table @ weights\n",
    "print(f'Dot products: {dot_products}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: define the threshold activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: compute the output based on the threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: 1\n",
      "Activation: 0\n",
      "Activation: 0\n",
      "Activation: 0\n"
     ]
    }
   ],
   "source": [
    "T = 0\n",
    "for i in range(0,4):\n",
    "    activation = linear_threshold_gate(dot_products[i], T)\n",
    "    print(f'Activation: {activation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time, we obtain the expected behavior: when both, Jackson and Tarantino are absent, the neuron fires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Limitations of the McCulloch-Pitts Artificial Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other boolean functions can be emulated with this simple, yet versatile model. Nonetheless, it naturally has many limitations that at the time were not as obvious as we know today. Among the main ones:\n",
    "- **Only binary inputs and outputs are allowed**: this is a significant limitation since many of the features that you can imagine can be useful to make decisions are *continuous* rather than binary. The same goes for the decisions themselves, wherein many instances you may want to attach a continuous value to a decision instead of a yes or no label.\n",
    "- **No learning is possible**: as you may notice, you have to figure it out the solution to your problem *beforehand* via *analysis*. In this sense, the model has no autonomy whatsoever, restricting the problems that can be solved to the ones that you know how to solve already. \n",
    "- **Manual adjustment of the weights and threshold**: connected to the lack of a learning procedure, once you figure out the solution, you will have to adjust all the parameters by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The McCulloch-Pitts artificial neuron was a significant first step in the development of artificial neural network models of cognition. Interestingly, it lacks two elements that are at the core of modern artificial neural networks: (1) it works with binary inputs and outputs, instead of arbitrary real-valued numbers, and (2) it does not incorporate a learning algorithm, which limits its functionality to problems that can be derived by the modeler. That being said, this model was a remarkable creative achievement to the extent that blended insights from theoretical computer science, logic, and neuroscience.\n",
    "\n",
    "An important remaining question is to what extent the MP artificial neuron says something substantial about human cognition. Although is true that the MP model was able to perform logical inference as humans do, it is not clear that the procedure is indeed similar to what is going on in the human mind. Part of the credibility of this approach comes from the fact that it is based on the observed behavior of biological neurons. If you come from the perspective that cognition is whatever the brain is doing, you have a starting point to validate this model. Now, binary threshold functions are an extremely simplified model of what brains do. Is this a good enough model of neurocognitive function? Probably not. As we know today, the behavior of ensembles of neurons is incredibly complex, to the point that it is hard to even conceive a complete model of brain function. That being said, it would be unfair to judge this model against such standards. As we mentioned in the introduction, computational models of cognition do not pretend to be a replica of the mind. From that perspective, it is undeniable that the MP model provided important insights into how to use this approach to better understand human cognition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fausett, L. (1994). When Neural Networks Began: The McCulloch-Pitts Neuron. In Fundamentals of neural networks: Architectures, algorithms, and applications (pp. 26–37). Prentice-Hall, Inc.\n",
    "- McCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4), 115-133.\n",
    "- Rojas, R. (2013). Threshold Logic. In Neural networks: A systematic introduction (pp. 29–52). Springer Science & Business Media."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
