---
interact_link: content/features/adaline.ipynb
kernel_name: python3
kernel_path: content/features
has_widgets: false
title: |-
  The ADALINE
pagenum: 7
prev_page:
  url: /features/perceptron.html
next_page:
  url: /features/multilayer-perceptron.html
suffix: .ipynb
search: adaline function learning training weights linear vector perceptron rule algorithm w compute widrow sum output sse figures set activation hat y eta difference between cost value class hoff step real x weight update values gradient errors using wj partial random pass after stanford means neural net network defined j where expected labels l rate surface yi implementation loop entire adaptive taking works squares mentioned input cases end point predicted comparing label e descent climbing hill python predictions matrix remember need same lets niter iteration test well tall wider problem grow networks b m ted yet artificial interested idea brain working

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The ADALINE</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Adaline algorithm (Adaptive Linear Algorithm) was proposed in 1959, shortly after Rosenblattâ€™s perceptron, by <strong>Bernard Widrow</strong> and <strong>Ted Hoff</strong> (one of the inventors of the microprocessor) at Stanford. Widrow and Hoff were electrical engineers, yet Widrow had attended the famous <em>Dartmouth Summer Research Project on Artificial Intelligence</em> in 1956, experience that got him interested in the idea of building brain-like artificial learning systems. When Widrow moved from MIT to Stanford, a colleague asked him whether he would be interested in taking Ted Hoff as his doctoral student. Widrow and Hoff came up with the Adaline algorithm on a Friday during their first session working together. At that time, implementing an algorithm in a mainframe computer was slow and expensive, so they decided to build a small device capable of being trained by the Adaline algorithm to learn to classify patterns of inputs.</p>
<p>The main difference between the perceptron and Adaline, is that Adaline works by minimizin the <strong>sum of squares of the linear errors</strong> over a training set. This means that the learning rule is based on a <strong>linear activation function</strong> rather than a unit step function as in the perceptron. This is important as allows the minimization of a continuous cost function. Continuous cost functions have the advantage of being differentiable, which allows training neural nets by using the chain rule of calculus, which opened the door to train more complex algorithms like non-linear multilayer perceptrons, logistic regression, support vector machines, and others.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Formal-definition-of-ADALINE">Formal definition of ADALINE<a class="anchor-link" href="#Formal-definition-of-ADALINE"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we mentioned before, Adaline uses a <strong>linear activation function</strong>, which is essencially the <strong>identity function</strong> of the net input to the network. This is defined as:</p>
$$\hat{y} = \sum_{j=1}^n x_j w_j + \theta$$<p>where:</p>
<ul>
<li>$\hat{y}$ is the output of the model (real value scalar)  </li>
<li>$x$ is a real value input vector   </li>
<li>$w$ is a real vaue weight vector</li>
<li>$\theta$ is a bias term  </li>
</ul>
<p>It is crucial to notice that even when we use a linear activation function to compute the output of the network, if we attempt to make a <strong>binary classification decision</strong>, we will still use a step-like decision function by taking the sign as:</p>
$$  sgn(\hat{y}) =
\begin{cases}
 +1,  &amp; \text{if $\hat{y}$ &gt; 0} \\
-1, &amp; \text{otherwise}
\end{cases}
$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-note-on-the-Perceptron-and-Adaline-fundamental-difference">A note on the Perceptron and Adaline fundamental difference<a class="anchor-link" href="#A-note-on-the-Perceptron-and-Adaline-fundamental-difference"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this point you may be wondering what's the difference between the Perceptron and Adaline considering that both end up using a step-function to make classifications. The difference is the <strong>learning rule to update the weight</strong> of the network. The perceptron update the weights by computing the difference between the expected and predicted <strong>class labels</strong>. In practice, this means that the perceptron is comparing three types of discrete values: -1, 0, and 1. On the other hand, Adaline computes the difference between the expected class label (i.e., -1 or 1) and the <strong>continious real value</strong> of the linear activation function.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-rule">Learning rule<a class="anchor-link" href="#Learning-rule"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we mentioned before, the Adaline learning rule consist of comparing the expected class label to the predicted continous real value output. To achieve this, Adaline uses the <strong>LMS (least mean square) algorithm</strong>, also know as <strong>Widrow-Hoff Delta Rule</strong>, which minimize the sum of squares of the linear errors over the training set. In the machine learning literature, this is know as a <strong>cost funtion</strong> to be minimized. This is defined as:</p>
$$L = \sum_{j=1}^n (\hat{y}_j-y_j)^2$$<p>where:</p>
<ul>
<li>$\hat{y}$ is the output of the model (real value scalar)  </li>
<li>$y$ is the expected class label (-1 or +1)</li>
</ul>
<p>Now the question is how to minimize sum of squares erros (SSE). We do this by adjusting the values of $w$ vector. Since we are working with a continous value function, we can compute the change in the SSE with respect to changes in $w$ by applying the <strong>gradient descent algorithm</strong>. Therefore, we update the values of $w$ by:</p>
$$w_{j+1} \leftarrow w_j + \eta(- \Delta_j)$$<p>where:</p>
<ul>
<li>$\eta$ is the learning rate (positive constant)</li>
<li>$\Delta_j$ is the value of the gradient at a point in the SSE surface </li>
</ul>
<p>This algorithm works by taking steps of a size controled by the learning rate $\eta$, on the surface defined by the vector of weights. A common way to express this idea is in analogy to climbing: if you're in a mountain, you can ascent by <strong>climbing up-hill</strong> or descent by <strong>climbing down-hill</strong>. Since the surface defined by this quadratic function is convex (think in a bowl) and has a unique global minimun, <strong>we want to go down-hill</strong> (i.e., we do <em>gradient descent</em>) where the SSE is minimized.</p>
<p>To obtain the gradient in a given point the convex-surface, we compute the partial derivative of the cost function $L$ with respect to each weight in the weight vector as:</p>
$$\frac{\partial L} {\partial w_j} = -\sum_{i}(y_i - \hat{y_i})x_{ji}$$<p></p>
<p>Finally, by replacing terms, the update rule can be writen as:</p>
$$\Delta w_j = -\eta \frac{\partial L} {\partial w_j} = \eta\sum_{i}(y_i - \hat{y_i})x_{ji}$$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adaline-algorithm-implementation">Adaline algorithm implementation<a class="anchor-link" href="#Adaline-algorithm-implementation"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will implement the Adaline algorithm from scrath with Python and Numpy (a Python package for scientific computing). The goal is to understand the perceptron step-by-step execution rather than achieving an elegant implementation. I'll break down each step into functions to ensemble everything at the end.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generate-vector-of-random-weights">Generate vector of random weights<a class="anchor-link" href="#Generate-vector-of-random-weights"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">random_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;create vector of random weights</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X: 2-dimensional array, shape = [n_samples, n_features]</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    w: array, shape = [w_bias + n_features]&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">w</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Predictions from Adaline are obtained by a linear combination of features and weights. It is common practice to begin with a vector of small random weights that would be updated later by the Adaline learning rule.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compute-net-input">Compute net input<a class="anchor-link" href="#Compute-net-input"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Compute net input as dot product&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we pass the featue matrix and the previously generated vector of random weights to compute the inner product. Remember that we need to add an extra weight for the bias term at the begining of the vector (<code>w[0</code>)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compute-activation">Compute activation<a class="anchor-link" href="#Compute-activation"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Compute linear activation&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">X</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that the activation function returns the same values passed in. As we mentioned earlier, the linear activation function of Adaline, is the <strong>identity function</strong>, which means exactly this: units will be activated in direct proportion to the output of the linear combination of vectors and weights. Technically, we might not use this function and the result will be the same. Yet, we add this for conceptual completeness.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compute-predictions">Compute predictions<a class="anchor-link" href="#Compute-predictions"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Return class label after unit step&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Remember that although Adaline learning rule works by comparing the output of a linear function against the class labels, when doing predictions, we still need to pass the output by a <em>sgn function</em> to get class labels as in the perceptron.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-loop---Learning-rule">Training loop - Learning rule<a class="anchor-link" href="#Training-loop---Learning-rule"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;loop over exemplars and update weights&#39;&#39;&#39;</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">random_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="n">net_input_v</span> <span class="o">=</span> <span class="n">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">net_input_v</span><span class="p">)</span> <span class="c1"># identity function</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span> <span class="c1"># compute errors for the entire dataset</span>
        <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">errors</span> <span class="c1"># update weigths for the entire dataset (feature weights)</span>
        <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="c1"># update weigths for the entire dataset (bias-term weights)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">errors</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="mf">2.0</span> 
        <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
 
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">costs</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's examine the fit method that implements the Adaline learning rule:</p>
<ul>
<li>Create a vector of random weights by using the <code>random_weights</code> function with dimensionality equal to the number of columns in the feature matrix</li>
<li>Loop over the entire dataset <code>n_iter</code> times with <code>for i in range(n_iter)</code></li>
<li>Compute the inner product between the feature matrix $X$ and the weight vector $w$ by using the <code>net_input(X, w)</code> function</li>
<li>Compute the difference between the predicted values and the target values for the entire dataset <code>(y - output)</code></li>
<li>Update the weights in proportion to the learning rate $\eta$ by <code>w[1:] += eta * X.T @ errors</code> and <code>w[0] += eta * errors.sum()</code></li>
<li>Compute the SSE <code>cost = (errors**2).sum() / 2.0</code></li>
<li>Save the SSE for each iteration <code>costs.append(cost)</code></li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Testing-the-Adaline">Testing the Adaline<a class="anchor-link" href="#Testing-the-Adaline"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will test the Adaline implementation on the same test that in our previous perceptron: <strong>classifying figures by their shape</strong>. We'll create two type of figures: <strong>tall-figures</strong> and <strong>wide-figures</strong>. As the name suggest, the tall-figures are figures that are taller than wider, and the wider-figures are figures that are wider than taller.</p>
<p>To accomplish this, we'll sample tall and wide figures at random from a normal distribution by using the following function:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define function to create figures type</span>
<span class="k">def</span> <span class="nf">figure_type</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;creates [n_sampes, 2] array</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mu1, sigma1: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-1, standar-dev feature-1</span>
<span class="sd">    mu2, sigma2: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-2, standar-dev feature-2</span>
<span class="sd">    n_samples: int, shape= [n_samples, 1]</span>
<span class="sd">        number of sample cases</span>
<span class="sd">    target: int, shape = [1]</span>
<span class="sd">        target value</span>
<span class="sd">    seed: int</span>
<span class="sd">        random seed for reproducibility</span>
<span class="sd">    </span>
<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    X: ndim-array, shape = [n_samples, 2]</span>
<span class="sd">        matrix of feature vectors</span>
<span class="sd">    y: 1d-vector, shape = [n_samples, 1]</span>
<span class="sd">        target vector</span>
<span class="sd">    ------</span>
<span class="sd">    X&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">f2</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create tall-figures matrix</span>
<span class="n">T</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{T.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_t.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{T[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_t[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[13.24869073  4.55287144]
 [ 8.77648717  6.2245077 ]
 [ 8.9436565   5.40349164]
 [ 7.85406276  5.59357852]] 
target vector: 
[1 1 1 1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create wide-figures matrix</span>
<span class="n">W</span><span class="p">,</span> <span class="n">y_w</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{W.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_w.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{W[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_w[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[ 4.58324215 12.32304298]
 [ 4.94373317 10.7721561 ]
 [ 2.8638039   7.73373345]
 [ 6.64027081 10.86618511]] 
target vector: 
[-1 -1 -1 -1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tall-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wide-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/adaline_29_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_t</span><span class="p">,</span> <span class="n">y_w</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train.shape: </span><span class="si">{X_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_train.shape: </span><span class="si">{y_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_test.shape: </span><span class="si">{X_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_test.shape: </span><span class="si">{y_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>X_train.shape: (140, 2)
y_train.shape: (140,)
X_test.shape: (60, 2)
y_test.shape: (60,)
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function takes the training set, training targets, learning rate, and number of iterations. Recall that the Adaline update the weights <strong>after</strong> doing a full pass for the entire training set, whereas the peceptron does it after each individual training exemplar. Therefore, we will pass the entire training set 1 time (<code>n_iter=1</code>)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit and predict values</span>
<span class="k">def</span> <span class="nf">training_values</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">w</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">n_iter</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y_pred</span>

<span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">training_values</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vector of weights: </span><span class="si">{w}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cost at each time step: </span><span class="si">{cost}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted value for each case: </span><span class="si">{y_pred}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>vector of weights: [ 0.02614765  0.42163998 -0.2764838 ]

cost at each time step: [70.66186787674164]

predicted value for each case: [ 1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1 -1 -1 -1  1  1  1  1  1  1  1
  1 -1  1  1  1  1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1 -1  1
 -1 -1  1 -1  1  1  1 -1 -1 -1 -1 -1]

</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">acc</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">num_correct_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_correct_predictions</span> <span class="o">/</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: </span><span class="si">%.2f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Test set accuracy: 93.33%
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We obtain a SSE (cost) equal to <strong>70.7</strong> with a <strong>93.33%</strong> of classification accuracy. Pretty good. Let's try now by passing training set <strong>twice times</strong> by the training loop.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit and predict values</span>
<span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">training_values</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vector of weights: </span><span class="si">{w}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cost at each time step: </span><span class="si">{cost}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted value for each case: </span><span class="si">{y_pred}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>vector of weights: [-0.12486643 -1.09800089 -1.08582971]

cost at each time step: [70.66186787674164, 158.3678895769619]

predicted value for each case: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]

</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: </span><span class="si">%.2f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Test set accuracy: 50.00%
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Something really odd happened: the <strong>SSE went up from 70 to 158.4</strong>, and the <strong>accuracy drop from 93.33% to 50%</strong>. All the test cases has been classified as -1. Maybe we need more training, let's try with 5 loops.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit and predict values</span>
<span class="n">w</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">training_values</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vector of weights: </span><span class="si">{w}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cost at each time step: </span><span class="si">{cost}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted value for each case: </span><span class="si">{y_pred}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">acc</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: </span><span class="si">%.2f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>vector of weights: [ 553.09756457 4250.98926834 4373.14848469]

cost at each time step: [70.66186787674164, 158.3678895769619, 20245.308891118297, 5041394.106643893, 1260698508.9443183]

predicted value for each case: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]

Test set accuracy: 50.00%
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-exploding-gradient-problem">The exploding gradient problem<a class="anchor-link" href="#The-exploding-gradient-problem"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Nothing improved, on the contrary, the SSE is getting bigger with each iteration over the training set. The key to this problem is on the <strong>vector of weights</strong>. Remember that the weights are updated in relation to the <strong>mistmatch between the net ouput of the linear activation function and the class labels</strong>. This means that unless the error is exactly zero, <strong>the weights will continue to grow (or decrease) without limit</strong> after each pass of the training set. As the weights grow,the net output will continue to grow as well, making the error on each iteration bigger. This loop continues <strong>until the weights "explode"</strong> and the network is unable to learn anything meaningful anymore. This is know as the "exploding" gradient problem, which is one of the main limitations on the use of this kind of training algorithms in the context of neural networks.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Linear-Separability-Constrain">The Linear Separability Constrain<a class="anchor-link" href="#The-Linear-Separability-Constrain"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#### TODO ####</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><p>Widrow, B., &amp; Hoff, M. E. (1960). Adaptive switching circuits (No. TR-1553-1). Stanford Univ Ca Stanford Electronics Labs.</p>
</li>
<li><p>Widrow, B., &amp; Lehr, M. A. (1990). 30 years of adaptive neural networks: perceptron, madaline, and backpropagation. Proceedings of the IEEE, 78(9), 1415-1442.</p>
</li>
<li><p>Widrow, B., &amp; Lehr, M. A. (1995). Perceptrons, Adalines, and backpropagation. The handbook of brain theory and neural networks, 719-724.</p>
</li>
</ul>
<p><strong>For code implementation:</strong></p>
<ul>
<li>Raschka, S. (2015). Python machine learning. Packt Publishing Ltd.</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    