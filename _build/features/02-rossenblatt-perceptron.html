---
interact_link: content/features/02-rossenblatt-perceptron.ipynb
kernel_name: python3
kernel_path: content/features
has_widgets: false
title: |-
  The Perceptron
pagenum: 4
prev_page:
  url: /features/01-MP-artificial-neuron.html
next_page:
  url: /features/03-adaline.html
suffix: .ipynb
search: perceptron learning vector figures weights w value function delta information compute between product algorithm theorist where feature linear rule inner equation wk such code work connections brain step begin end model predicted implementation random rosenblatt ideas empiricist connectionist sensory stimulus system simple very defined decision training x bias term line plane case eta rate weight create proposed form storage takes into neurons organism input class threshold z cases otherwise net update process error python later matrix lets using xi errors further tall wider data knowledge wanted questions stored contained memory influence recognition mapping pattern inspired nervous e rather others formally

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The Perceptron</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Frank Rosenblatt proposed the 'perceptron' algorithm as a way to formalize several ideas about knowledge representation and cognition in the mid 20th century (Rossenblatt, 1958). He wanted to approach two fundamental questions:</p>
<ul>
<li><strong>In what form is information stored or remembered?</strong> </li>
<li><strong>How does the information contained in storage, or in memory, influence recognition and behavior?</strong></li>
</ul>
<p>According to Rossenblat, there were two distinct approaches to answer such questions: the <strong>code theorist</strong> and the <strong>empiricist/connectionist theorist</strong>. <strong>Code theorist</strong> proposed that sensory information is in the form of <em>coded representations</em> or images, with some sort of one-to-one mapping between the sensory stimulus and the stored pattern. On the other hand, <strong>empiricist/connectionist theorist</strong>, inspired by how biological systems actually work, proposed that the central nervous system acts as an intricate <em>switching network</em>, where information takes the form of new connections or pathways between centers of activity. In other words, accoding to empiricist/connectionist theorist, there is no simple one-to-one mapping from stimulus into memory, i.e., <em>the information or knowledge is contained in the connections or associations between neurons</em>, rather than as static blueprints to be compared with incoming sensory stimulus.</p>
<p>Rosenblatt, inspired by the work of empiricist/connectionist theorist like Hebb, Hayek, and others, wanted to express these ideas more formally and rigorously in order to be tested computationally. Some key ideas guiding Rosenblatt reasoning where:</p>
<ul>
<li>The physical connections in the brain involved in learning and recognition vary from organism to organism</li>
<li>Plasticity is a key feature of the nervous system, such that changes in response to experience</li>
<li>Rewards and punishments can increase or decrease the probability of certain connections reacting to similar stimulus</li>
</ul>
<p>Therefore, information is understood as a dynamic ever-changing pattern of connectivity among neurons in the brain, which are subject to the influence of experience and the environment of the organism. As such, no two brains are the same, and the way in which your brain and may brain storage information about the same fact may be very completely different.</p>
<p>The perceptron is a formal mathematical manifestation of these ideas. In essence, a <strong>single-layer perceptron</strong> is a system that:</p>
<ul>
<li>Takes and input through the sensory system (e.g., the retina)</li>
<li>Combines the values of the inputs with the weights of the connections between the neurons</li>
<li>Produce an output in an all-or-nothing fashion belonging to a class of objects (mathematically, as a step-function). </li>
</ul>
<p>Next, we formally define a perceptron and implement the algorithm in code.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Formal-definition-of-the-perceptron-algorithm">Formal definition of the perceptron algorithm<a class="anchor-link" href="#Formal-definition-of-the-perceptron-algorithm"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mathematically, the perceptron is a simple <strong>linear classifier</strong> defined by two formulas</p>
<ul>
<li>a <strong>decision or threshold function</strong></li>
<li>a <strong>training or learning rule</strong></li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Decision-function">Decision function<a class="anchor-link" href="#Decision-function"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <strong>decision or threshold function</strong> for the perceptron is defined as:</p>
$$  f(z) =
\begin{cases}
+1,  &amp; \text{if $w$ $\cdot$ $x$ + $b$ &gt; 0} \\
-1, &amp; \text{otherwise}
\end{cases}
$$<p>where:</p>
<ul>
<li>${z}$ is a vector of reald-valued features  </li>
<li>$w$ is a vector of real-valued weights  </li>
<li>$w \cdot x$ is a inner product   </li>
<li>$b$ is the bias term  </li>
</ul>
<p><strong>Note about the bias term</strong>: we add a bias term to allow the model to adjust the intercept of the line on the plane. Otherwise, the line would be forced to go through the origin in the cartesian plane (in the 2-Dimensional case), which might hinder the ability of the model to find the best line.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To compute the <strong>net-input</strong> for the decision function, we need to compute the inner product (or dot product) of the feature vector and the weigths vector. The inner product is defined as:</p>
\begin{equation*}
z = \sum_{k=1}^n w_k x_k = w_0 x_0 + w_1 x_1 + ... + w_n x_n
\end{equation*}
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learning-rule">Learning rule<a class="anchor-link" href="#Learning-rule"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The perceptron learning rule is simple: compute the mistmatch between the <strong>predicted value</strong> for a given exemplar its <strong>actual value</strong>, and then use such delta to update the value the weights vector. This is defined as:</p>
\begin{equation*}
w_k = w_k + \Delta w_k
\end{equation*}<p>The $\Delta w_k$ is computed as:</p>
\begin{equation*}
\Delta w_k = \eta(y^i - \hat{y}^i)x^i_k
\end{equation*}<p>where:</p>
<ul>
<li>$\eta$ is the learning rate (0 -1 value)</li>
<li>$y^i$ is the actual value ("true class")</li>
<li>$\hat{y}^i$ is the predicted value ("predicted class")</li>
<li>$x^i_k$ is the feature vector for case $k$</li>
</ul>
<p><strong>Note about the learning rate $\eta$</strong>: the learning rate has the role of facilitating the training process by weighting the delta used to update the weights. This basically means that instead of completely replacing the previous weight with the sum of the weight + delta, we incorporate a <strong>proportion</strong> of the error into the updating process. This makes the learning process more stable and smooth.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Perceptron-algorithm-implementation">Perceptron algorithm implementation<a class="anchor-link" href="#Perceptron-algorithm-implementation"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will implement the perceptron algorithm from scrath with Python and Numpy (a Python package for scientific computing). The goal is to understand the perceptron step-by-step execution rather than achieving an elegant implementation. I'll break down each step into functions to ensemble everything at the end.</p>
<p>The scikit-learn implementation can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html">here</a>. In general, we don't want to re-implemented algorithms that has been thouroughly tested by others, to avoid duplication of work an potential bugs in our code.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generate-random-weights-vector">Generate random weights vector<a class="anchor-link" href="#Generate-random-weights-vector"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">random_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;create vector of random weights</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X: 2-dimensional array, shape = [n_samples, n_features]</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    w: array, shape = [w_bias + n_features]&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span> <span class="o">+</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">w</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Predictions from the percetron are obtained by a linear combination of features and weights. It is common practice to begin with a vector of small random weights that would be updated later by the perceptron learning rule.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compute-net-input">Compute net input<a class="anchor-link" href="#Compute-net-input"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Compute net input as dot product&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we pass the featue matrix and the previously generated vector of random weights to compute the inner product. Remember that we need to add an extra weight for the bias term at the begining of the vector (<code>w[0</code>)</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Compute-predictions">Compute predictions<a class="anchor-link" href="#Compute-predictions"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Return class label after unit step&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">net_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This method implements the <strong>threshold function</strong> which takes the net-value of the inner product and outputs a 1 if the predicted value is &gt;= 0, and -1 otherwise.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Training-loop---Learning-rule">Training loop - Learning rule<a class="anchor-link" href="#Training-loop---Learning-rule"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;loop over exemplars and update weights&#39;&#39;&#39;</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">random_weights</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">exemplar</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
        <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">xi</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">eta</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">predict</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">w</span><span class="p">))</span>
            <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">xi</span>
            <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">delta</span>
            <span class="n">error</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">delta</span> <span class="o">!=</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">errors</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's examine the fit method that implements the learning rule:</p>
<ul>
<li>Create a vector of random weights by using the <code>random_weights</code> function with dimensionality equal to the number of columns in the feature matrix.</li>
<li>Loop over each row of the feature matrix with <code>for exemplar in range(n_iter)</code></li>
<li>Compute the inner product between the feature vector for row $i$ and the weight vector by using the <code>predict(xi, w)</code> function</li>
<li>Compute the difference between the predicted value and the target value times the learning rate with <code>delta = eta * (target - predict(xi, w))</code></li>
<li>Update the weights by <code>w[1:] += delta * xi</code> and <code>w[0] += delta</code></li>
<li>We also save the errors for further plotting <code>errors.append(error)</code></li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Testing-the-perceptron">Testing the perceptron<a class="anchor-link" href="#Testing-the-perceptron"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we will test the implementation by creating a simple example: <strong>classifying figures by their shape</strong>. We'll create two type of figures: <strong>tall-figures</strong> and <strong>wide-figures</strong>. As the name suggest, the tall-figures are figures that are taller than wider, and the wider-figures are figures that are wider than taller.</p>
<p>To accomplish this, we'll sample tall and wide figures at random from a normal distribution by using the following function:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define function to create figures type</span>
<span class="k">def</span> <span class="nf">figure_type</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;creates [n_sampes, 2] array</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mu1, sigma1: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-1, standar-dev feature-1</span>
<span class="sd">    mu2, sigma2: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-2, standar-dev feature-2</span>
<span class="sd">    n_samples: int, shape= [n_samples, 1]</span>
<span class="sd">        number of sample cases</span>
<span class="sd">    target: int, shape = [1]</span>
<span class="sd">        target value</span>
<span class="sd">    seed: int</span>
<span class="sd">        random seed for reproducibility</span>
<span class="sd">    </span>
<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    X: ndim-array, shape = [n_samples, 2]</span>
<span class="sd">        matrix of feature vectors</span>
<span class="sd">    y: 1d-vector, shape = [n_samples, 1]</span>
<span class="sd">        target vector</span>
<span class="sd">    ------</span>
<span class="sd">    X&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">f2</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create tall-figures matrix</span>
<span class="n">T</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{T.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_t.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{T[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_t[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[13.24869073  4.55287144]
 [ 8.77648717  6.2245077 ]
 [ 8.9436565   5.40349164]
 [ 7.85406276  5.59357852]] 
target vector: 
[1 1 1 1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create wide-figures matrix</span>
<span class="n">W</span><span class="p">,</span> <span class="n">y_w</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{W.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_w.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{W[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_w[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[ 4.58324215 12.32304298]
 [ 4.94373317 10.7721561 ]
 [ 2.8638039   7.73373345]
 [ 6.64027081 10.86618511]] 
target vector: 
[-1 -1 -1 -1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To make this clearer, we can create a plot to visualize each figure</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tall-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wide-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/02-rossenblatt-perceptron_28_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_t</span><span class="p">,</span> <span class="n">y_w</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train.shape: </span><span class="si">{X_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_train.shape: </span><span class="si">{y_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_test.shape: </span><span class="si">{X_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_test.shape: </span><span class="si">{y_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>X_train.shape: (140, 2)
y_train.shape: (140,)
X_test.shape: (60, 2)
y_test.shape: (60,)
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># fit and predict values</span>
<span class="n">w</span><span class="p">,</span> <span class="n">errors</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">20</span> <span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vector of weights: </span><span class="si">{w}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;errors at each time step: </span><span class="si">{errors}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;predicted value for each case: </span><span class="si">{y_pred}{nl}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>vector of weights: [ 0.01624345  0.84235904 -0.76528073]

errors at each time step: [14, 6, 4, 2, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 2]

predicted value for each case: [ 1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1 -1 -1 -1  1 -1  1  1  1  1  1
  1 -1  1  1 -1  1  1 -1 -1 -1 -1  1 -1  1  1 -1 -1 -1 -1  1  1  1 -1  1
 -1 -1  1 -1 -1  1  1 -1 -1 -1 -1 -1]

</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By printing the prediction error ar each time step we can see that the perceptron predicts each case correctly after 12 training cicles. Let's plot the errors to complement the analysis.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 

<span class="k">def</span> <span class="nf">plot_errors</span><span class="p">(</span><span class="n">errors</span><span class="p">):</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">))</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;training-step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;error&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_errors</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/02-rossenblatt-perceptron_33_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_correct_predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_correct_predictions</span> <span class="o">/</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set accuracy: </span><span class="si">%.2f%%</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Test set accuracy: 98.33%
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Linear-Separability-Constrain">The Linear Separability Constrain<a class="anchor-link" href="#The-Linear-Separability-Constrain"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, the perceptron reach ~98% of classification accuracy very fast. Yet, it can't get to 100%. The reason is that perceptron, as linear model, only can perfectly solve 'linearly separable' problems. In the case of our figures dataset, there some small amount of overlap, a few figures that by pure chance are very similar in the 2-D plane of vertical-distance and horizontal-distance. Let's look at the graph again.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tall-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wide-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/02-rossenblatt-perceptron_37_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Near the coordinates (vertical=5, horizontal=5) there are a few data points that can't be separated by fitting a line to the plane because their are almost overlapping. If we create a data set where the data points are further separated, we could get a linear model like the perceptron to work and perfectly classify each instance, but those cases are very unusual and limit the scope of problems that can be approached. This limitation later had a huge impact in the use and research of artificial neural networks, particularly after 1969 when Minsky and Papert published their book 'Perceptrons', where they criticize and show the limitations of  Rosenblatt's work. Later on,  research in artificial neural networks would overcome such limitations by the integration of a combination of multiple layers of processing units, non-linear units, and the backpropagation algorithm.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References-and-Further-Reading">References and Further Reading<a class="anchor-link" href="#References-and-Further-Reading"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.</li>
</ul>
<p><strong>For code implementation:</strong></p>
<ul>
<li>Raschka, S. (2015). Python machine learning. Packt Publishing Ltd.</li>
</ul>

</div>
</div>
</div>
</div>

 


    </main>
    