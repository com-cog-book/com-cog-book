---
interact_link: content/features/multilayer-perceptron.ipynb
kernel_name: python3
kernel_path: content/features
has_widgets: false
title: |-
  The Multilayer Perceptron
pagenum: 8
prev_page:
  url: /features/adaline.html
next_page:
  url: /LICENSE.html
suffix: .ipynb
search: perceptron multilayer linear output s units layers weights layer hidden error compute w equation z network not function begin end learning problem non neural algorithm multiple input x networks backpropagation bmatrix derivatives adaline functions patterns such before binary features decision step adding take mathrm activity derivative therefore processing unit xor y model e threshold pass vector sigmoid value respect limitations via rumelhart hinton williams different rule between architecture inputs idea cognitive simple boundary case artificial through activation predictions change problems name because training neuron method instead generating single core science perceptrons center seems easy solve same why learn possible boundaries

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The Multilayer Perceptron</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-objectives">Learning objectives<a class="anchor-link" href="#Learning-objectives"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Understand the principles behind the creation of the multilayer perceptron</li>
<li>Identify how the multilayer perceptron overcame many of the limitations of previous models</li>
<li>Expand understanding of learning via gradient descent methods</li>
<li>Develop a basic code implementation of the multilayer perceptron in Python</li>
<li>Determine what kind of problems can and can't be solved with the multilayer perceptron</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Historical-and-theoretical-background">Historical and theoretical background<a class="anchor-link" href="#Historical-and-theoretical-background"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Truth be told, "multilayer perceptron" is a terrible name for what Rumelhart, Hinton, and Williams introduced in 1985. It is a bad name because its most fundamental piece, the training algorithm, is completely different to the one in the perceptron. Therefore, a multilayer perceptron it is not simply "a perceptron with multiple layers", as the name may suggest. True, it is a network composed my multiple neuron-like processing units, but not every neuron-like processing unit is a perceptron. If you were to put together a bunch of Rossenblat's perceptron in sequence, you would obtain something very different to what most people today would call a multilayer perceptron.</p>
<p>Confusing naming aside...</p>
<p>As the ADALINE, the multilayer perceptron uses a gradient descent method for training, instead of the delta-rule method of the perceptron. Therefore, it is not</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Perceptron and Adaline algorithms are characterized by generating a <strong>direct mapping between input and output patterns</strong>. Such characteristic is a direct result of their <strong>single-layer architecture</strong>. With one layer, there is no place to generate <strong>intermediate representations</strong> of the inputs before mapping to the outputs. The idea of <strong>generating internal representations of the entities from the outside world is a core notion in cognitive science</strong>: the mind and the brain must do some form of internal processing and reorganization of the information captured from the environment, that allows things like reasoning, language, and complex thinking and behavior more generally. Hence, it is hardly surprising that a single-layer Perceptron or Adaline could not recreate many human cognitive capacities. Marvin Minsky and Seymour Papert devoted an entire book (‘Perceptrons’, 1969) to show the many limitations of Perceptrons as learning algorithms. One classical example of the limitations of single-layer architectures with linear units is the <strong>exclusive-or (XOR) problem</strong> illustrated in <em>Table 1</em> below:</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>Table 1</center><table>
<thead><tr>
<th>Input patterns</th>
<th>-&gt;</th>
<th>Output Patterns</th>
</tr>
</thead>
<tbody>
<tr>
<td>00</td>
<td>-&gt;</td>
<td>0</td>
</tr>
<tr>
<td>01</td>
<td>-&gt;</td>
<td>1</td>
</tr>
<tr>
<td>10</td>
<td>-&gt;</td>
<td>1</td>
</tr>
<tr>
<td>11</td>
<td>-&gt;</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The XOR problem seems to be a <strong>trivially easy to solve for humans</strong>: when the input patterns are the same, output 1; when they are different, output 0. There is a simple graphical way to show why a Perceptron can’t learn to solve such a simple problem as shown in the <em>Figure 1</em> below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>Figure 1</center><p><img src="images/XOR_linear.png" width="60%"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The X-axis and Y-axis reflect the combination of binary features in the XOR problem (0 or 1). When X = Y = 0, the output should be 0, when X = 1 and Y = 0, the output should be 1. The dashed orange line represents the <strong>decision boundary</strong> generated by the Perceptron. The red 1s and 0s represent incorrect classifications; the green 1s and 0s represent correct classifications. As we can see, there is <strong>no way to generate a decision boundary capable of classifying all the instances correctly</strong> (you can keep trying all possible straight lines, and won't work). The box in the lower right corner shows a case where all instances are correctly classified, but to do that we need two boundaries instead of one. There are two possible approaches to tackle this problem: (1) <strong>changing the representation of the problem</strong>, this is to say, generating some sort of transformation of the inputs into a space that is linearly separable, or (2) using a model that is not restricted to linear decision boundaries, i.e., <strong>allowing for non-linearities in the model</strong>.</p>
<p>Since changing the representation of a problem involves some form of transformation of the inputs, an intuitive first step could be just to modify the input features by hand. For instance, we could try a coding scheme like the one in <em>Table 2</em>. In such a case, we are adding a third dimension that actually allows separating the 1s and 0s with a plane (linear model). Yet, this seems to be a very unsatisfactory solution since it requires <strong>hand-crafting features</strong> which <strong>do not generalize</strong> well beyond toy problems.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center>Table 2</center><table>
<thead><tr>
<th>Input patterns</th>
<th>-&gt;</th>
<th>Output Patterns</th>
</tr>
</thead>
<tbody>
<tr>
<td>000</td>
<td>-&gt;</td>
<td>0</td>
</tr>
<tr>
<td>010</td>
<td>-&gt;</td>
<td>1</td>
</tr>
<tr>
<td>100</td>
<td>-&gt;</td>
<td>1</td>
</tr>
<tr>
<td>111</td>
<td>-&gt;</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A second approach could be just adding more layers to the Perceptron. Unfortunately, <strong>no matter how many layers we add, the output of a linear model, is a linear decision boundary</strong>. Therefore, that is not a viable solution. A combination of <strong>adding more layers and allowing for non-linearities</strong> in the model seems to be the way to go, and that was precisely what Rumelhart, Hinton, and Williams did in 1985.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A-new-generation-of-artificial-neural-networks">A new generation of artificial neural networks<a class="anchor-link" href="#A-new-generation-of-artificial-neural-networks"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Even though Rumelhart, Hinton, and Williams called this architecture 'Multilayer-Perceptron', the learning procedure differs significantly from the Perceptron: the Perceptron learning rule operates over a binary threshold function, whereas <strong>the Multilayer-Perceptron operates over the linear or (non-linear) output <em>before</em> passing the output through the binary threshold function</strong> for prediction (in the case of binary classification).</p>
<p>The Multilayer-Perceptron differs from the Adaline as well. Widrow and Hoff did come up with the idea of creating a multilayer Adaline, known as the MADALINE (Many Adalines). Yet, the Adaline and MADALINE are limited to <strong>strictly linear units</strong>, and strictly linear units are subject to the linear separability constraint. Rumelhart, Hinton, and Williams overcome such limitations by adding <strong>non-linear</strong> processing units to the Multilayer-Perceptron architecture.</p>
<p>Before going further, it is important to think on <strong>why is that concatenating multiple layers with non-linear units works</strong>. As we saw before, hand-crafted features can help a linear system to solve the XOR problem, but such solutions do not scale up to larger problems. In essence, adding multiple layers of processing units is a way to <strong>automate the so-called feature engineering</strong> process. With multiple layers, now is the <strong>network itself search for new features in an iterative fashion</strong>, and that happens in the hidden layers. Additionally, allowing for non-linear activation functions <strong>expands the set of possible features to be learned</strong> beyond linear boundaries. For instance, in the case of the XOR problem, a non-linear decision boundary could be learned automatically. This is why modern artificial neural networks are considered <strong>universal function approximators</strong>: with enough computation and data, a neural network can approximate functions of any shape.</p>
<p>Mathematically speaking, the remaining challenge was to find a way to <strong>efficiently adjust the weights across the multiple layers</strong> of the network. One option is to randomly guess values for the weights, then evaluate the error, and repeat until a good enough solution has been found. Unfortunately, this approach is extremely slow and inefficient, particularly for large networks with millions of weights to be adjusted. Fortunately, as we saw with the Adaline, there is a better way: adjusting the weights via <strong>backpropagation</strong>, i.e., the <strong>chain-rule of calculus</strong>. We can use this method, because Multilayer-perceptrons use <strong>continuous functions instead of binary threshold functions</strong>, which are differentiable using the rules of calculus.</p>
<p>The application of the backpropagation algorithm in multilayer neural network architectures was a major breakthrough in the artificial intelligence and cognitive science community, that catalyzed a new generation of research in cognitive science. Nonetheless, it took several decades of advance on computing and data availability before artificial neural networks became the dominant paradigm in the research landscape as it is today.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Formal-definition-of-the-multilayer-perceptron-with-non-linear-units">Formal definition of the multilayer perceptron with non-linear units<a class="anchor-link" href="#Formal-definition-of-the-multilayer-perceptron-with-non-linear-units"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The precise mathematical definition of a multilayer perceptron will depend on the chosen architecture, this is to say, how many layers, how many units, whether it is fully connected, etc. Let's take a simple fully connected neural network with two input neurons in the <strong>input layer</strong>, three neurons in the <strong>hidden layer</strong>, and one neuron in the <strong>output layer</strong>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/forward_neural_net_2_inputs.png" width="60%"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's unpack the math from the forward pass of information to make predictions.</p>
<p><strong>Step 1: Matrix multiplication input to hidden layer</strong></p>
$$\begin{bmatrix} 
w_{11} &amp; w_{12} &amp; w_{13} \\ 
w_{21} &amp; w_{22} &amp; w_{23} \\ 
\end{bmatrix}
\begin{bmatrix} 
x_{1} \\ 
x_{2} \\
x_{3}
\end{bmatrix}
$$<p>We first take the dot product of the matrix of weights ($w_{ij}$ with the vector of inputs ($x_i$) to compute $z_i$ as</p>
$$\begin{equation}
z_{i} = \sum_{j=1}^{3} x_j w_{ij}
\end{equation}$$<p><strong>Step 2: Pass $z_i$ through the sigmoid activation function $S(z)$ in the hidden layer</strong></p>
$$\begin{equation}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} }
\end{equation}
$$<p></p>
<p>We use the value of $z_i$ as a parameter for the sigmoid function in each node</p>
<p><strong>Step 3: Multiplication output vector of the hidden layer with vector for weights</strong></p>
$$\begin{bmatrix} 
w_{11} &amp; w_{12} &amp; w_{13} \\ 
\end{bmatrix}
\begin{bmatrix} 
s_{1} \\ 
s_{2} \\
s_{3}
\end{bmatrix}
$$<p>We take the dot product between the output vector from the hidden layer and the vector of weights connecting to the output layer.</p>
<p><strong>Step 4: Pass $z$ through the sigmoid activation function $S(z)$ in the output layer</strong></p>
$$\begin{equation}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} }
\end{equation}
$$<p></p>
<p>Now we obtain our predicted value $\hat{y}$</p>
<p><strong>Step 5: Pass $\hat{y}$ through the binary decision function $f(S)$</strong></p>
$$ 
\begin{equation}
f(S) = \begin{cases} 1, &amp; \mbox{if S(z)&gt;0} \\ 0, &amp; \mbox{otherwise} \end{cases}
\end{equation}
$$<p>Finally, we classify as $0$ or $1$ based on the value of $f(S)$</p>
<p>All these steps <strong>roughly describe the core of the forward pass of most neural networks</strong>. More complex models like convolutional and recurrent nets implement essentially the same steps with different activation functions and links between layers.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-backpropagation-learning-algorithm">The backpropagation learning algorithm<a class="anchor-link" href="#The-backpropagation-learning-algorithm"> </a></h2><p>We have seen how neural networks take data and make predictions. Now the question is how neural networks learn to make better predictions. As we mentioned, this is accomplished via the backpropagation algorithm. At its core, backpropagation is a clever way to <strong>automate the search for parameters</strong> (the arrows connecting the layers in our graph) that minimize the aggregate error of the predictions.</p>
<p>The goal of the backpropagation algorithm is to compute <strong>how fast the error changes as we change the activity of the weights in the network</strong>. A few relatively simple operations are used:</p>
<ol>
<li><strong>compute the error</strong> between the predicted and expected value, in other words, the value of the so-called <strong>loss function</strong></li>
<li><strong>compute the error derivatives</strong> in the output layer, and from there </li>
<li><strong>compute the error derivatives in the layers that came before</strong>. In other words, the chain-rule of calculus throughout the layers. And finally,</li>
<li><strong>compute the error derivatives of the weights</strong></li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-algorithm">Learning algorithm<a class="anchor-link" href="#Learning-algorithm"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notes in how to learn the weights of a logistic unit:</p>
<ul>
<li>As we mentioned, adding layers of linear units does not change the learning capacities of a neural network, thefore, we will examine the learning algorithm procedure for a multilayer network with non-linear units directly.</li>
<li>With hidden units, we face non-convex optimization problem, where the error surface defined by the weights, it is not shaped like a bowl. Therefore, you can easily get stuck in a local minima.</li>
<li>We know how to compute the derivatives to adjust the weights in a network withouth hidden units. Unfortunately, such procedure does not generalize to multilayer networks. The challenge is to find a way to train the network</li>
<li>Threshold functions won't work, because their derivative is infinite at the threshold, and zero elsewhere.</li>
<li>The fact that sigmoid functions change smoothly and constiniously, generates nice derivative which make learning easy.</li>
<li>To train the network, we first need to compute the derivative of the output with respect to the logit and the derivative of the logit with respect to the weights. Then, we can compute the derivative of the output with respect to the weights.</li>
</ul>
<p>As we know, a linear function is defined as:
\begin{equation*}
z = \sum_{k=1}^n w_k x_k = w_0 x_0 + w_1 x_1 + ... + w_n x_n
\end{equation*}</p>
<p>The sigmoid function, will take the output <strong><em>z</em></strong> from the linear function as follow:</p>
\begin{equation*}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} } 
\end{equation*}<p>HERE: equation that take the derivative of the error with respect to the weights</p>
<p>HERE: Explain how this generalize to training multiple layers using the backpropagation algorithm</p>
<ul>
<li>The key idea behind backpropagation is to compute how fast the error changes, as we change the <strong>activity</strong> of the hidden units.</li>
<li>This means that we don't try to compute the 'ideal' activity of a hidden unit, but to compute the error derivatives with respect to the activity of the hidden units.</li>
<li>Since the activity of a hidden unit, may affect the activity of many output units, and therefore, to affect the overall error via multiple outputs.</li>
<li>The idea is to compute the error derivatives for all the hidden units <strong>at the same time</strong>. Once we have those derivatives, is easy to compute the error derivatives for the weights going into a hidden unit.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;z&#39;</span><span class="p">:</span><span class="n">z</span><span class="p">,</span> <span class="s1">&#39;S(z)&#39;</span><span class="p">:</span><span class="n">s</span><span class="p">})</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>

<span class="n">lin</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;z&#39;</span>
<span class="p">)</span>

<span class="n">sig</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;S(z)&#39;</span>
<span class="p">)</span>


<span class="n">lin</span> <span class="o">|</span> <span class="n">sig</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/features/multilayer-perceptron_20_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multilayer-perceptron-implementation">Multilayer perceptron implementation<a class="anchor-link" href="#Multilayer-perceptron-implementation"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: </span>
<span class="c1"># - create a function to generate training data for XOR problem</span>
<span class="c1"># - plot the training data</span>
<span class="c1"># - set up keras model </span>
<span class="c1"># - train and test keras model</span>
<span class="c1"># - compute metrics</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define function to create figures type</span>
<span class="k">def</span> <span class="nf">figure_type</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;creates [n_sampes, 2] array</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mu1, sigma1: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-1, standar-dev feature-1</span>
<span class="sd">    mu2, sigma2: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-2, standar-dev feature-2</span>
<span class="sd">    n_samples: int, shape= [n_samples, 1]</span>
<span class="sd">        number of sample cases</span>
<span class="sd">    target: int, shape = [1]</span>
<span class="sd">        target value</span>
<span class="sd">    seed: int</span>
<span class="sd">        random seed for reproducibility</span>
<span class="sd">    </span>
<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    X: ndim-array, shape = [n_samples, 2]</span>
<span class="sd">        matrix of feature vectors</span>
<span class="sd">    y: 1d-vector, shape = [n_samples, 1]</span>
<span class="sd">        target vector</span>
<span class="sd">    ------</span>
<span class="sd">    X&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">f2</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create tall-figures matrix</span>
<span class="n">T</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{T.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_t.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{T[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_t[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[13.24869073  4.55287144]
 [ 8.77648717  6.2245077 ]
 [ 8.9436565   5.40349164]
 [ 7.85406276  5.59357852]] 
target vector: 
[1 1 1 1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create wide-figures matrix</span>
<span class="n">W</span><span class="p">,</span> <span class="n">y_w</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{W.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_w.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{W[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_w[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[ 4.58324215 12.32304298]
 [ 4.94373317 10.7721561 ]
 [ 2.8638039   7.73373345]
 [ 6.64027081 10.86618511]] 
target vector: 
[-1 -1 -1 -1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tall-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wide-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/multilayer-perceptron_27_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_t</span><span class="p">,</span> <span class="n">y_w</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train.shape: </span><span class="si">{X_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_train.shape: </span><span class="si">{y_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_test.shape: </span><span class="si">{X_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_test.shape: </span><span class="si">{y_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>X_train.shape: (140, 2)
y_train.shape: (140,)
X_test.shape: (60, 2)
y_test.shape: (60,)
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span> <span class="c1"># create custom activation function here</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

 


    </main>
    