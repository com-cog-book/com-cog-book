---
interact_link: content/features/multilayer-perceptron.ipynb
kernel_name: python3
kernel_path: content/features
has_widgets: false
title: |-
  The Multilayer Perceptron
pagenum: 8
prev_page:
  url: /features/adaline.html
next_page:
  url: /LICENSE.html
suffix: .ipynb
search: neural perceptron units backpropagation layers not s network networks hidden compute multilayer weights w equation z error algorithm book linear layer begin end com cog output learning idea rumelhart multiple function derivatives bmatrix science mathematical hinton research nets processing adaline features learn does take mathrm activity derivative limitations gradient problems cognitive work different change train github io html unit non problem data pass step x vector sigmoid value respect models artificial thought group eventually introduced training between therefore finally capacity representations same predictions e via descent kind came researchers intelligence too took into pdp became important called today threshold such

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">The Multilayer Perceptron</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-objectives">Learning objectives<a class="anchor-link" href="#Learning-objectives"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Understand the principles behind the creation of the multilayer perceptron</li>
<li>Identify how the multilayer perceptron overcame many of the limitations of previous models</li>
<li>Expand understanding of learning via gradient descent methods</li>
<li>Develop a basic code implementation of the multilayer perceptron in Python</li>
<li>Determine what kind of problems can and can't be solved with the multilayer perceptron</li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Historical-and-theoretical-background">Historical and theoretical background<a class="anchor-link" href="#Historical-and-theoretical-background"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="The-origin-of-the-backpropagation-algorithm">The origin of the backpropagation algorithm<a class="anchor-link" href="#The-origin-of-the-backpropagation-algorithm"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Neural networks research came close to become an anecdote in the history of cognitive science by 1980. The vast majority of researchers in cognitive science and artificial intelligence thought that neural nets were a silly idea, they could not possibly work. Minsky and Papert even provided formal proofs about it 1969. Yet, as any person that has been around science long enough knows, there are plenty of stubborn researchers that will continue paddling against the current in pursue of their own ideas.</p>
<p>David Rumelhart first heard about perceptrons and neural nets in 1963 while in graduate school at Stanford. At the time, he was doing research in mathematical psychology, which although it has lots of equations, is a different field, so he did not pay too much attention to neural nets. It wasn't until the early 70's that Rumelhart took neural nets more seriously. He was in pursuit of a more general framework to understand cognition. Mathematical psychology looked too much like a disconnected mosaic of ad-doc formulas for him. By the late '70s, Rumelhart was working at UC San Diego. He and some colleagues formed a study group about neural networks in cognitive science, that eventually evolved into what is known as the "Parallel Distributed Processing" (PDP) research group. Among the members of that group were Geoffrey Hinton, Terrence Sejnowski, Michael I. Jordan, Jeffrey L. Elman, Francis Crick, and others that eventually became prominent researchers in the neural networks and artificial intelligence fields.</p>
<p>The original intention of the PDP group was to create a compendium of the most important research on neural networks. Their enterprise eventually evolved into something larger, producing the famous two volumes book where the so-called "backpropagation" algorithm was introduced, along with other important models and ideas. Although most people today associate the invention of the gradient descent algorithm to Hinton, the person that came up the idea was David Rumelhart, and as in most things in science, it was just a small change to a previous idea. Rumelhart and James McClelland (another young professor at UC San Diego) wanted to train a neural network with multiple layers and sigmoidal units, instead of threshold units (as in the perceptron) or linear units (as in the ADALINE), but they did not how to train such a model. Rumelhart knew that you could use gradient descent to train networks with linear units, as Widrow and Hoff did, so he thought that he might as well pretend that sigmoids units were linear units and see what happens. It worked, but he realized that training the model took too many iterations, so the got discouraged and let the idea aside for a while.</p>
<p>Backpropagation remained dormant for a couple of years until Hinton picked it up again. Rumelhart introduced the idea to Hinton, and Hinton thought it was a terrible idea. I could not work. He knew that backpropagation could not break the symmetry between weights and it will get stuck in local minima. He was passionate about energy-based systems known as <a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf">Boltzmann machines</a>, which seemed to have nicer mathematical properties. Yet, as he failed to solve more and more problems with Boltzmann machines, decided to try out backpropagation, mostly out of frustration. It worked amazingly well, way better than Boltzmann machines. He got in touch with Rumelhart about their results, and both decided to include a backpropagation chapter in the PDP book, and published <em>Nature</em> paper along with Ronald Williams. The <em>Nature</em> paper became highly visible and the interest in neural networks got reignited for at least the next decade. And that is how backpropagation was introduced: by a mathematical psychologist with no training in computational modeling and a neural net researcher that thought it was a terrible idea.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overcoming-limitations-and-creating-advantages">Overcoming limitations and creating advantages<a class="anchor-link" href="#Overcoming-limitations-and-creating-advantages"> </a></h3>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Truth be told, "multilayer perceptron" is a terrible name for what Rumelhart, Hinton, and Williams introduced in the mid-'80s. It is a bad name because its most fundamental piece, the <em>training algorithm</em>, is completely different from <a href="https://com-cog-book.github.io/com-cog-book/features/perceptron.html#Learning-procedure">the one in the perceptron</a>. Therefore, a multilayer perceptron it is not simply "a perceptron with multiple layers", as the name suggests. True, it is a network composed of multiple neuron-like processing units, but not every neuron-like processing unit is a perceptron. If you were to put together a bunch of Rossenblat's perceptron in sequence, you would obtain something very different from what most people today would call a multilayer perceptron.</p>
<p>Now, the main reason for the resurgence of interest in neural networks was that finally, someone designed an architecture that could overcome the perceptron and ADALINE limitations: <em>to efficiently solve problems requiring non-linear solutions</em>. Problems like the famous <a href="https://com-cog-book.github.io/com-cog-book/features/perceptron.html#Example-1:-the-XOR-problem">XOR (exclusive or)</a> function (to learn more about it, see the "Limitations" section in the <a href="https://com-cog-book.github.io/com-cog-book/features/perceptron.html#Example-1:-the-XOR-problem">"The Perceptron"</a> and <a href="https://com-cog-book.github.io/com-cog-book/features/adaline.html#ADALINE-limitations">"The ADALINE"</a> chapters).</p>
<p>Further, a side effect of the capacity to use multiple layers of non-linear units, is that neural networks can form <em>complex internal representations of entities</em>. The perceptron and ADALINE did not have this capacity. They both are linear models, therefore, it doesn't matter how many layers of processing units you concatenate together, the representation learned by the network will be a linear model. You may as well have nonadditional layers and the network eventually would learn the same solution with multiple layers (see <a href="https://com-cog-book.github.io/com-cog-book/features/perceptron.html#Why-adding-multiple-layers-of-processing-units-does-not-work">Why adding multiple layers of processing units does not work</a> for an explanation). This capacity is important in so far complex multi-level representation of phenomena is -probably- what the human mind does when solving problems in language, perception, learning, etc.</p>
<p>Finally, the backpropagation algorithm effectively automates the so-called feature engineering process. If you have ever done data analysis of any kind, you may have come across variables or features that were not in the original data, but that was <em>created by transforming or combining other variables</em>. For instance, you may have variables for income and education, and combine those to create a socio-economic status variable. That variable may have a predictive capacity above and beyond income and education in isolation. With a multi-layer neural network with non-linear units trained with backpropagation, such a <em>transformation process happens automatically</em> in the intermediate or "hidden" layers of the network. Those intermediate representations often aren't hard or impossible to interpret for humans. They may make no sense whatsoever for us, but somehow help to solve the pattern recognition problem at hand, so the network will learn that representation. Does this mean that neural nets learn different representations from the human brain? Maybe, maybe not. The problem is that we don't have direct access to the kind of representations learned by the brain either, and a neural net will seldom be trained with the same data that a human brain is trained in real life.</p>
<p>The application of the backpropagation algorithm in multilayer neural network architectures was a major breakthrough in the artificial intelligence and cognitive science community, that catalyzed a new generation of research in cognitive science. Nonetheless, it took several decades of advance on computing and data availability before artificial neural networks became the dominant paradigm in the research landscape as it is today. Next, we will explore its mathematical formalization and application.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mathematical-formalization">Mathematical formalization<a class="anchor-link" href="#Mathematical-formalization"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The precise mathematical definition of a multilayer perceptron will depend on the chosen architecture, this is to say, how many layers, how many units, whether it is fully connected, etc. Let's take a simple fully connected neural network with two input neurons in the <strong>input layer</strong>, three neurons in the <strong>hidden layer</strong>, and one neuron in the <strong>output layer</strong>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="images/forward_neural_net_2_inputs.png" width="100%"></p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's unpack the math from the forward pass of information to make predictions.</p>
<p><strong>Step 1: Matrix multiplication input to hidden layer</strong></p>
$$\begin{bmatrix} 
w_{11} &amp; w_{12} &amp; w_{13} \\ 
w_{21} &amp; w_{22} &amp; w_{23} \\ 
\end{bmatrix}
\begin{bmatrix} 
x_{1} \\ 
x_{2} \\
x_{3}
\end{bmatrix}
$$<p>We first take the dot product of the matrix of weights ($w_{ij}$ with the vector of inputs ($x_i$) to compute $z_i$ as</p>
$$\begin{equation}
z_{i} = \sum_{j=1}^{3} x_j w_{ij}
\end{equation}$$<p><strong>Step 2: Pass $z_i$ through the sigmoid activation function $S(z)$ in the hidden layer</strong></p>
$$\begin{equation}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} }
\end{equation}
$$<p></p>
<p>We use the value of $z_i$ as a parameter for the sigmoid function in each node</p>
<p><strong>Step 3: Multiplication output vector of the hidden layer with vector for weights</strong></p>
$$\begin{bmatrix} 
w_{11} &amp; w_{12} &amp; w_{13} \\ 
\end{bmatrix}
\begin{bmatrix} 
s_{1} \\ 
s_{2} \\
s_{3}
\end{bmatrix}
$$<p>We take the dot product between the output vector from the hidden layer and the vector of weights connecting to the output layer.</p>
<p><strong>Step 4: Pass $z$ through the sigmoid activation function $S(z)$ in the output layer</strong></p>
$$\begin{equation}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} }
\end{equation}
$$<p></p>
<p>Now we obtain our predicted value $\hat{y}$</p>
<p><strong>Step 5: Pass $\hat{y}$ through the binary decision function $f(S)$</strong></p>
$$ 
\begin{equation}
f(S) = \begin{cases} 1, &amp; \mbox{if S(z)&gt;0} \\ 0, &amp; \mbox{otherwise} \end{cases}
\end{equation}
$$<p>Finally, we classify as $0$ or $1$ based on the value of $f(S)$</p>
<p>All these steps <strong>roughly describe the core of the forward pass of most neural networks</strong>. More complex models like convolutional and recurrent nets implement essentially the same steps with different activation functions and links between layers.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-backpropagation-learning-algorithm">The backpropagation learning algorithm<a class="anchor-link" href="#The-backpropagation-learning-algorithm"> </a></h2><p>We have seen how neural networks take data and make predictions. Now the question is how neural networks learn to make better predictions. As we mentioned, this is accomplished via the backpropagation algorithm. At its core, backpropagation is a clever way to <strong>automate the search for parameters</strong> (the arrows connecting the layers in our graph) that minimize the aggregate error of the predictions.</p>
<p>The goal of the backpropagation algorithm is to compute <strong>how fast the error changes as we change the activity of the weights in the network</strong>. A few relatively simple operations are used:</p>
<ol>
<li><strong>compute the error</strong> between the predicted and expected value, in other words, the value of the so-called <strong>loss function</strong></li>
<li><strong>compute the error derivatives</strong> in the output layer, and from there </li>
<li><strong>compute the error derivatives in the layers that came before</strong>. In other words, the chain-rule of calculus throughout the layers. And finally,</li>
<li><strong>compute the error derivatives of the weights</strong></li>
</ol>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-algorithm">Learning algorithm<a class="anchor-link" href="#Learning-algorithm"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notes in how to learn the weights of a logistic unit:</p>
<ul>
<li>As we mentioned, adding layers of linear units does not change the learning capacities of a neural network, thefore, we will examine the learning algorithm procedure for a multilayer network with non-linear units directly.</li>
<li>With hidden units, we face non-convex optimization problem, where the error surface defined by the weights, it is not shaped like a bowl. Therefore, you can easily get stuck in a local minima.</li>
<li>We know how to compute the derivatives to adjust the weights in a network withouth hidden units. Unfortunately, such procedure does not generalize to multilayer networks. The challenge is to find a way to train the network</li>
<li>Threshold functions won't work, because their derivative is infinite at the threshold, and zero elsewhere.</li>
<li>The fact that sigmoid functions change smoothly and constiniously, generates nice derivative which make learning easy.</li>
<li>To train the network, we first need to compute the derivative of the output with respect to the logit and the derivative of the logit with respect to the weights. Then, we can compute the derivative of the output with respect to the weights.</li>
</ul>
<p>As we know, a linear function is defined as:
\begin{equation*}
z = \sum_{k=1}^n w_k x_k = w_0 x_0 + w_1 x_1 + ... + w_n x_n
\end{equation*}</p>
<p>The sigmoid function, will take the output <strong><em>z</em></strong> from the linear function as follow:</p>
\begin{equation*}
S(z) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-z} } 
\end{equation*}<p>HERE: equation that take the derivative of the error with respect to the weights</p>
<p>HERE: Explain how this generalize to training multiple layers using the backpropagation algorithm</p>
<ul>
<li>The key idea behind backpropagation is to compute how fast the error changes, as we change the <strong>activity</strong> of the hidden units.</li>
<li>This means that we don't try to compute the 'ideal' activity of a hidden unit, but to compute the error derivatives with respect to the activity of the hidden units.</li>
<li>Since the activity of a hidden unit, may affect the activity of many output units, and therefore, to affect the overall error via multiple outputs.</li>
<li>The idea is to compute the error derivatives for all the hidden units <strong>at the same time</strong>. Once we have those derivatives, is easy to compute the error derivatives for the weights going into a hidden unit.</li>
</ul>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">altair</span> <span class="k">as</span> <span class="nn">alt</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;z&#39;</span><span class="p">:</span><span class="n">z</span><span class="p">,</span> <span class="s1">&#39;S(z)&#39;</span><span class="p">:</span><span class="n">s</span><span class="p">})</span>

<span class="n">base</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>

<span class="n">lin</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;z&#39;</span>
<span class="p">)</span>

<span class="n">sig</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;S(z)&#39;</span>
<span class="p">)</span>


<span class="n">lin</span> <span class="o">|</span> <span class="n">sig</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="../images/features/multilayer-perceptron_14_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Multilayer-perceptron-implementation">Multilayer perceptron implementation<a class="anchor-link" href="#Multilayer-perceptron-implementation"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># TODO: </span>
<span class="c1"># - create a function to generate training data for XOR problem</span>
<span class="c1"># - plot the training data</span>
<span class="c1"># - set up keras model </span>
<span class="c1"># - train and test keras model</span>
<span class="c1"># - compute metrics</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Define function to create figures type</span>
<span class="k">def</span> <span class="nf">figure_type</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;creates [n_sampes, 2] array</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mu1, sigma1: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-1, standar-dev feature-1</span>
<span class="sd">    mu2, sigma2: int, shape = [n_samples, 2]</span>
<span class="sd">        mean feature-2, standar-dev feature-2</span>
<span class="sd">    n_samples: int, shape= [n_samples, 1]</span>
<span class="sd">        number of sample cases</span>
<span class="sd">    target: int, shape = [1]</span>
<span class="sd">        target value</span>
<span class="sd">    seed: int</span>
<span class="sd">        random seed for reproducibility</span>
<span class="sd">    </span>
<span class="sd">    Return</span>
<span class="sd">    ------</span>
<span class="sd">    X: ndim-array, shape = [n_samples, 2]</span>
<span class="sd">        matrix of feature vectors</span>
<span class="sd">    y: 1d-vector, shape = [n_samples, 1]</span>
<span class="sd">        target vector</span>
<span class="sd">    ------</span>
<span class="sd">    X&#39;&#39;&#39;</span>
    <span class="n">rand</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">f2</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n_samples</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create tall-figures matrix</span>
<span class="n">T</span><span class="p">,</span> <span class="n">y_t</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{T.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_t.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{T[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_t[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[13.24869073  4.55287144]
 [ 8.77648717  6.2245077 ]
 [ 8.9436565   5.40349164]
 [ 7.85406276  5.59357852]] 
target vector: 
[1 1 1 1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># create wide-figures matrix</span>
<span class="n">W</span><span class="p">,</span> <span class="n">y_w</span> <span class="o">=</span> <span class="n">figure_type</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">nl</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix shape: </span><span class="si">{W.shape}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector shape: </span><span class="si">{y_w.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Feature matrix: </span><span class="si">{nl}{W[0:4, :]}</span><span class="s1"> </span><span class="si">{nl}</span><span class="s1">target vector: </span><span class="si">{nl}{y_w[0:4]}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Feature matrix shape: (100, 2) 
target vector shape: (100,)
Feature matrix: 
[[ 4.58324215 12.32304298]
 [ 4.94373317 10.7721561 ]
 [ 2.8638039   7.73373345]
 [ 6.64027081 10.86618511]] 
target vector: 
[-1 -1 -1 -1]
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;tall-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">W</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">W</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;wide-figures&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="../images/features/multilayer-perceptron_21_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_t</span><span class="p">,</span> <span class="n">y_w</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_train.shape: </span><span class="si">{X_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_train.shape: </span><span class="si">{y_train.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X_test.shape: </span><span class="si">{X_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_test.shape: </span><span class="si">{y_test.shape}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>X_train.shape: (140, 2)
y_train.shape: (140,)
X_test.shape: (60, 2)
y_test.shape: (60,)
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;random_uniform&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span> <span class="c1"># create custom activation function here</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;rmsprop&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using TensorFlow backend.
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Kelley, H. J. (1960). Gradient theory of optimal flight paths. Ars Journal, 30(10), 947–954.</p>
<p>Bryson, A. E. (1961). A gradient method for optimizing multi-stage allocation processes. Proc. Harvard Univ. Symposium on Digital Computers and Their Applications, 72.</p>
<p>Werbos, P. J. (1994). The roots of backpropagation: From ordered derivatives to neural networks and political forecasting (Vol. 1). John Wiley &amp; Sons.</p>

</div>
</div>
</div>
</div>

 


    </main>
    